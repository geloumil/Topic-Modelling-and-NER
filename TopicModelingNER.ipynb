{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"#5642C2\" size=\"5\">Excersise 3 in Natural Language Processing</font>\n",
    "<br>\n",
    "<font color=\"#5642C2\" size=\"4\">Topic Modeling and NER</font>\n",
    "<br>\n",
    "<font color=\"#5642C2\" size=\"3\">Author:</font><font color=\"#ADADAD\" size=\"3\">Angeliki Mylonaki</font>\n",
    "<br>\n",
    "<font color=\"#5642C2\" size=\"3\">Github:</font> https://github.com/geloumil/Topic-Modelling-and-NER.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'str'>\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import codecs\n",
    "\n",
    "def getData(regex):\n",
    "\n",
    "    file_list=[]\n",
    "    trainFiles = glob.glob(os.path.join(\".\", regex)) #make list of paths\n",
    "\n",
    "    for fd in trainFiles:\n",
    "        with codecs.open(fd, encoding='utf-8', errors='ignore') as fp:\n",
    "            file_list.append(fp.read().encode(\"utf-8\"))\n",
    "            \n",
    "    return file_list\n",
    "\n",
    "documents=getData(\"./nipstxt/nips01/*\")\n",
    "print type(documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"#5642C2\" size=\"3\">Mining Documents</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import corpus\n",
    "from nltk import word_tokenize\n",
    "from nltk import sent_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "def mineDocs(documents):\n",
    "    \n",
    "    porter_stemmer = PorterStemmer()\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    minedList=[]\n",
    "    \n",
    "    stopWords = corpus.stopwords.words(\"english\")\n",
    "    documents=[s.translate(None, string.punctuation) for s in documents]\n",
    "       \n",
    "    for doc in documents:\n",
    "         \n",
    "\n",
    "        \n",
    "        #splitting document into sentences\n",
    "        sentences = sent_tokenize(doc)\n",
    "        for sentence in sentences:\n",
    "            #removing digits\n",
    "            sentence=sentence.translate(None, string.digits)\n",
    "            \n",
    "            #getting words of phrase\n",
    "            words=word_tokenize(sentence)\n",
    "            \n",
    "            #stemming\n",
    "            words=[porter_stemmer.stem(word) for word in words]\n",
    "            words=[wordnet_lemmatizer.lemmatize(word) for word in words]\n",
    "            \n",
    "            #removing stopwords\n",
    "            sentence = ' '.join([word for word in words\n",
    "                             if word not in stopWords])\n",
    "            \n",
    "            minedList.append(sentence)\n",
    "            \n",
    "    return minedList\n",
    "\n",
    "\n",
    "\n",
    "mdocs=mineDocs(documents)\n",
    "mdocs_tokenized=[doc.split() for doc in mdocs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"#5642C2\" size=\"3\">Vectorizing,TF-IDF </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "count_vect = CountVectorizer(stop_words=\"english\", binary=\"true\")\n",
    "m_docs_vector = count_vect.fit_transform(mdocs)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(use_idf=False).fit(m_docs_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"#5642C2\" size=\"3\">Creating Document-term Matrix</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "# Creating the term dictionary of our courpus, where every unique term is assigned an index.\n",
    "dictionary = corpora.Dictionary(mdocs_tokenized)\n",
    "\n",
    "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in mdocs_tokenized]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"#5642C2\" size=\"3\">LDA</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the object for LDA model using gensim library\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# Running and Trainign LDA model on the document term matrix.\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=5, id2word = dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"#5642C2\" size=\"3\">Results</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, u'0.013*\"network\" + 0.010*\"thi\" + 0.007*\"learn\"'), (4, u'0.018*\"network\" + 0.011*\"use\" + 0.011*\"thi\"'), (3, u'0.010*\"thi\" + 0.009*\"network\" + 0.008*\"use\"')]\n"
     ]
    }
   ],
   "source": [
    "print(ldamodel.print_topics(num_topics=3, num_words=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
